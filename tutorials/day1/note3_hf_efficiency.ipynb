{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6989fd99",
   "metadata": {},
   "source": [
    "# Model Quantization with HuggingFace Transformers\n",
    "\n",
    "Quantization reduces memory usage and can improve throughput at a modest accuracy cost.\n",
    "\n",
    "## Why Quantization?\n",
    "- **Memory**: 4-bit models use ~1/4 the memory of fp16 models\n",
    "- **Speed**: Lower precision can enable faster inference\n",
    "- **Deployment**: Fit larger models on smaller GPUs\n",
    "\n",
    "## Quantization Methods:\n",
    "- **8-bit (INT8)**: Good balance, minimal accuracy loss\n",
    "- **4-bit (NF4)**: Maximum compression, some quality tradeoff\n",
    "- **GPTQ/AWQ**: Post-training quantization optimized for inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59eca862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: allocated 0.00 GB, reserved 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Set up torch for optimal performance\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.fp32_precision = \"tf32\"\n",
    "    torch.backends.cudnn.conv.fp32_precision = \"tf32\"\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Model identifiers\n",
    "MODEL_INSTRUCT = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "MODEL_THINKING = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
    "\n",
    "def report_memory(tag):\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available.\")\n",
    "        return\n",
    "    torch.cuda.synchronize()\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"{tag}: allocated {allocated:.2f} GB, reserved {reserved:.2f} GB\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "report_memory(\"Initial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b17af864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "def load_model(model_id, quantization_config=None):\n",
    "    \"\"\"Load a model and tokenizer with optional quantization.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        dtype=torch.bfloat16,\n",
    "        quantization_config=quantization_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43a9799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_messages(user_prompt, system_prompt=\"You are a helpful assistant.\"):\n",
    "    \"\"\"Format messages for chat models.\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "\n",
    "def generate_chat(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Generate a chat response.\"\"\"\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **kwargs,\n",
    "    )\n",
    "    if do_sample:\n",
    "        gen_kwargs.update({\"temperature\": temperature, \"top_p\": top_p})\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(**gen_kwargs)\n",
    "\n",
    "    gen_ids = output_ids[0, input_ids.shape[-1] :]\n",
    "    return tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "comparison_prompts = {\n",
    "    \"math\": \"Solve: If a train travels 120 km in 1.5 hours, what is its average speed?\",\n",
    "    \"reasoning\": \"You have 3 boxes: apples, oranges, and mixed. All labels are wrong. \"\n",
    "    \"Pick one fruit to identify all boxes. Explain.\",\n",
    "    \"analysis\": \"Compare pros and cons of deploying an LLM on-prem vs in the cloud.\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ff2ff",
   "metadata": {},
   "source": [
    "## Three-Way Comparison: Baseline vs INT8 vs NF4\n",
    "\n",
    "Run the same test prompts across all three quantization levels to compare outputs and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53156f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING: Full Precision (BF16)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f035401f4fc9405997a036293c4bd148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (FP16/BF16): allocated 8.05 GB, reserved 8.05 GB\n",
      "✓ Baseline results collected.\n",
      "\n",
      "============================================================\n",
      "LOADING: 8-bit Quantization (INT8)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6151be75df44cc7857f61cc4bc45c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT8 (8-bit): allocated 4.45 GB, reserved 4.67 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/rds/VipinRDS/VipinRDS/users/mxh1029/.cache/conda/envs/h/lib/python3.13/site-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ INT8 results collected.\n",
      "\n",
      "============================================================\n",
      "LOADING: 4-bit Quantization (NF4)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5643e1a1ab848f6979f7293da99fdf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NF4 (4-bit): allocated 2.70 GB, reserved 4.12 GB\n",
      "✓ NF4 results collected.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store results from all three quantization methods\n",
    "all_results = {}\n",
    "\n",
    "# 1. BASELINE (Full Precision)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING: Full Precision (BF16)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'model' in locals() and model is not None:\n",
    "    del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer, model = load_model(MODEL_INSTRUCT)\n",
    "report_memory(\"Baseline (FP16/BF16)\")\n",
    "\n",
    "baseline_results = {}\n",
    "for name, prompt in comparison_prompts.items():\n",
    "    messages = format_messages(prompt, system_prompt=\"You are a precise assistant.\")\n",
    "    baseline_results[name] = generate_chat(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        messages,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "all_results['Baseline (FP16)'] = baseline_results\n",
    "print(\"✓ Baseline results collected.\\n\")\n",
    "\n",
    "# 2. INT8 QUANTIZATION\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING: 8-bit Quantization (INT8)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'model' in locals() and model is not None:\n",
    "    del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bnb_8bit = BitsAndBytesConfig(load_in_8bit=True)\n",
    "tokenizer, model = load_model(MODEL_INSTRUCT, quantization_config=bnb_8bit)\n",
    "report_memory(\"INT8 (8-bit)\")\n",
    "\n",
    "int8_results = {}\n",
    "for name, prompt in comparison_prompts.items():\n",
    "    messages = format_messages(prompt, system_prompt=\"You are a precise assistant.\")\n",
    "    int8_results[name] = generate_chat(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        messages,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "all_results['INT8 (8-bit)'] = int8_results\n",
    "print(\"✓ INT8 results collected.\\n\")\n",
    "\n",
    "# 3. NF4 QUANTIZATION\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING: 4-bit Quantization (NF4)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'model' in locals() and model is not None:\n",
    "    del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bnb_4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer, model = load_model(MODEL_INSTRUCT, quantization_config=bnb_4bit)\n",
    "report_memory(\"NF4 (4-bit)\")\n",
    "\n",
    "nf4_results = {}\n",
    "for name, prompt in comparison_prompts.items():\n",
    "    messages = format_messages(prompt, system_prompt=\"You are a precise assistant.\")\n",
    "    nf4_results[name] = generate_chat(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        messages,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "all_results['NF4 (4-bit)'] = nf4_results\n",
    "print(\"✓ NF4 results collected.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c309520",
   "metadata": {},
   "source": [
    "### Display Results\n",
    "\n",
    "Compare outputs across all three quantization levels side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24a0d092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TASK: MATH\n",
      "======================================================================\n",
      "\n",
      "[Baseline (FP16)]\n",
      "----------------------------------------------------------------------\n",
      "To find the **average speed** of the train, use the formula:\n",
      "\n",
      "$$\n",
      "\\text{Average Speed} = \\frac{\\text{Total Distance}}{\\text{Total Time}}\n",
      "$$\n",
      "\n",
      "Given:\n",
      "- Distance = 120 km\n",
      "- Time = 1.5 hours\n",
      "\n",
      "$$\n",
      "\\text{Average Speed} = \\frac{120 \\text{ km}}{1.5 \\text{ hours}} = 80 \\text{ km/h}\n",
      "$$\n",
      "\n",
      "### ✅ Answer: **80 km/h**\n",
      "\n",
      "\n",
      "[INT8 (8-bit)]\n",
      "----------------------------------------------------------------------\n",
      "To find the **average speed** of the train, use the formula:\n",
      "\n",
      "$$\n",
      "\\text{Average Speed} = \\frac{\\text{Total Distance}}{\\text{Total Time}}\n",
      "$$\n",
      "\n",
      "Given:\n",
      "- Distance = 120 km\n",
      "- Time = 1.5 hours\n",
      "\n",
      "$$\n",
      "\\text{Average Speed} = \\frac{120 \\text{ km}}{1.5 \\text{ hours}} = 80 \\text{ km/h}\n",
      "$$\n",
      "\n",
      "### ✅ Answer: **80 km/h**\n",
      "\n",
      "\n",
      "[NF4 (4-bit)]\n",
      "----------------------------------------------------------------------\n",
      "To find the **average speed** of the train, use the formula:\n",
      "\n",
      "$$\n",
      "\\text{Average Speed} = \\frac{\\text{Total Distance}}{\\text{Total Time}}\n",
      "$$\n",
      "\n",
      "Given:\n",
      "- Distance = 120 km\n",
      "- Time = 1.5 hours\n",
      "\n",
      "$$\n",
      "\\text{Average Speed} = \\frac{120 \\text{ km}}{1.5 \\text{ hours}} = 80 \\text{ km/h}\n",
      "$$\n",
      "\n",
      "### ✅ Answer: **80 km/h**\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TASK: REASONING\n",
      "======================================================================\n",
      "\n",
      "[Baseline (FP16)]\n",
      "----------------------------------------------------------------------\n",
      "This is a classic logic puzzle! Let's break it down step by step.\n",
      "\n",
      "### Given:\n",
      "- There are **3 boxes**:  \n",
      "  - Apples  \n",
      "  - Oranges  \n",
      "  - Mixed (apples and oranges)  \n",
      "- **All labels are wrong**.  \n",
      "- Your goal: **Pick just one fruit** (from one box) to correctly identify the contents of **all three boxes**.\n",
      "\n",
      "---\n",
      "\n",
      "### Step-by-step reasoning:\n",
      "\n",
      "Since **all labels are wrong**, we know:\n",
      "- The box labeled \"Apples\" does **not** contain only apples.\n",
      "- The box labeled \"Oranges\" does **not** contain only oranges.\n",
      "- The box labeled \"Mixed\" does **not** contain a mix.\n",
      "\n",
      "So, we can use this contradiction to our advantage.\n",
      "\n",
      "---\n",
      "\n",
      "### Strategy:\n",
      "Pick **one fruit** from the box labeled **\"Mixed\"**.\n",
      "\n",
      "Why this box?\n",
      "\n",
      "Because it's labeled \"Mixed\", but **all labels are wrong**, so it **cannot**\n",
      "\n",
      "\n",
      "[INT8 (8-bit)]\n",
      "----------------------------------------------------------------------\n",
      "This is a classic logic puzzle! Let's break it down step by step.\n",
      "\n",
      "### Given:\n",
      "- There are **3 boxes**:\n",
      "  - Apples\n",
      "  - Oranges\n",
      "  - Mixed (both apples and oranges)\n",
      "- **All labels are wrong** — that means:\n",
      "  - The box labeled \"Apples\" does **not** contain only apples.\n",
      "  - The box labeled \"Oranges\" does **not** contain only oranges.\n",
      "  - The box labeled \"Mixed\" does **not** contain a mix.\n",
      "\n",
      "### Goal:\n",
      "Pick **one fruit** (from one box) to correctly identify the contents of **all three boxes**.\n",
      "\n",
      "---\n",
      "\n",
      "### Strategy:\n",
      "\n",
      "Since **all labels are wrong**, we can use this to our advantage.\n",
      "\n",
      "#### Step 1: Pick a fruit from the box labeled \"Mixed\"\n",
      "\n",
      "Why this box?\n",
      "\n",
      "- The label \"Mixed\" is **wrong**, so this box does **not** contain a mix.\n",
      "- Therefore, it must contain **only apples\n",
      "\n",
      "\n",
      "[NF4 (4-bit)]\n",
      "----------------------------------------------------------------------\n",
      "Yes! This is a classic logic puzzle. Let's go step by step.\n",
      "\n",
      "### The Setup:\n",
      "You have **three boxes**:\n",
      "- Labeled: **Apples**, **Oranges**, **Mixed**\n",
      "\n",
      "But — **all labels are wrong**.\n",
      "\n",
      "Your goal: **Pick just one fruit** (from one box) to correctly identify the contents of **all three boxes**.\n",
      "\n",
      "---\n",
      "\n",
      "### Step-by-step Explanation:\n",
      "\n",
      "#### Step 1: Understand the constraint\n",
      "All labels are **wrong**. That means:\n",
      "- The box labeled \"Apples\" does **not** contain only apples.\n",
      "- The box labeled \"Oranges\" does **not** contain only oranges.\n",
      "- The box labeled \"Mixed\" does **not** contain a mix (i.e., both apples and oranges).\n",
      "\n",
      "So, you can't trust any of the labels.\n",
      "\n",
      "---\n",
      "\n",
      "#### Step 2: Strategy\n",
      "You need to pick **one fruit** from **one box** to deduce the contents of all three.\n",
      "\n",
      "The\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TASK: ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "[Baseline (FP16)]\n",
      "----------------------------------------------------------------------\n",
      "Deploying a Large Language Model (LLM) either **on-premises (on-prem)** or **in the cloud** each comes with distinct advantages and disadvantages. The choice depends on organizational needs such as data sensitivity, compliance, cost, performance, and scalability. Below is a structured comparison of the **pros and cons** of each approach:\n",
      "\n",
      "---\n",
      "\n",
      "### ✅ **On-Premise Deployment (On-Prem)**\n",
      "\n",
      "#### **Pros:**\n",
      "1. **Data Privacy & Security**  \n",
      "   - Sensitive data (e.g., healthcare, financial, legal) remains within your organization’s network.  \n",
      "   - No data exposure to third-party providers.  \n",
      "   - Full control over access, encryption, and audit logs.\n",
      "\n",
      "2. **Compliance & Regulatory Alignment**  \n",
      "   - Easier to meet industry-specific regulations (e.g., HIPAA, GDPR, PCI-DSS, SOX).  \n",
      "   - Avoids compliance risks associated with cloud data residency or cross-border data\n",
      "\n",
      "\n",
      "[INT8 (8-bit)]\n",
      "----------------------------------------------------------------------\n",
      "Deploying a Large Language Model (LLM) on-premises (on-prem) versus in the cloud each comes with distinct advantages and disadvantages. The choice depends on organizational needs such as data sensitivity, cost, control, scalability, and technical capacity. Below is a structured comparison of the **pros and cons** of each approach:\n",
      "\n",
      "---\n",
      "\n",
      "### ✅ **On-Premise Deployment (Local/On-Prem)**\n",
      "\n",
      "#### **Pros:**\n",
      "1. **Data Privacy & Security**  \n",
      "   - Sensitive data (e.g., healthcare, financial, government) remains within the organization’s network.  \n",
      "   - Complies with strict regulations (e.g., HIPAA, GDPR, PCI-DSS).  \n",
      "   - No data exposure to third-party providers.\n",
      "\n",
      "2. **Full Control & Customization**  \n",
      "   - Full ownership over model configuration, updates, and infrastructure.  \n",
      "   - Ability to fine-tune models for domain-specific use cases (e.g., legal, medical\n",
      "\n",
      "\n",
      "[NF4 (4-bit)]\n",
      "----------------------------------------------------------------------\n",
      "Deploying a Large Language Model (LLM) can be done either **on-premises (on-prem)** or **in the cloud**. Each approach has distinct advantages and disadvantages depending on organizational needs, security, cost, scalability, and compliance requirements. Below is a detailed comparison of the **pros and cons** of each deployment model:\n",
      "\n",
      "---\n",
      "\n",
      "### ✅ **On-Premise Deployment (On-Prem)**\n",
      "\n",
      "#### **Pros:**\n",
      "1. **Data Privacy & Security**\n",
      "   - Sensitive data (e.g., PII, medical records, financials) never leaves the organization.\n",
      "   - Full control over data access, encryption, and audit trails.\n",
      "   - Ideal for regulated industries (e.g., healthcare, finance, government).\n",
      "\n",
      "2. **Compliance & Regulatory Alignment**\n",
      "   - Meets strict compliance requirements (e.g., HIPAA, GDPR, SOX, PCI-DSS).\n",
      "   - Avoids data residency or cross-border transfer issues.\n",
      "\n",
      "3. **\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display comprehensive three-way comparison\n",
    "for task_name in comparison_prompts:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TASK: {task_name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for quant_type, results in all_results.items():\n",
    "        print(f\"\\n[{quant_type}]\")\n",
    "        print(\"-\" * 70)\n",
    "        print(results[task_name])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a493e7a",
   "metadata": {},
   "source": [
    "### Memory Usage Summary\n",
    "\n",
    "Review the memory statistics printed during model loading:\n",
    "- **Baseline (BF16)**: Highest accuracy, maximum memory usage\n",
    "- **INT8**: ~50% memory reduction with minimal quality loss\n",
    "- **NF4 (4-bit)**: ~75% memory reduction with acceptable quality tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e269ab",
   "metadata": {},
   "source": [
    "## Other Quantization Formats\n",
    "\n",
    "Beyond bitsandbytes, several other quantization methods are available:\n",
    "\n",
    "#### GPTQ (GPU Post-Training Quantization)\n",
    "- Pre-quantized models available on HuggingFace Hub\n",
    "- Optimized for fast inference on GPU\n",
    "- Loading: Use `AutoGPTQForCausalLM` class or models with `-GPTQ` suffix\n",
    "\n",
    "#### AWQ (Activation-Aware Weight Quantization)\n",
    "- Preserves important weights based on activation magnitudes\n",
    "- Better accuracy than naive 4-bit quantization\n",
    "- Loading: Use `AutoAWQForCausalLM` class or models with `-AWQ` suffix\n",
    "\n",
    "#### GGUF (llama.cpp format)\n",
    "- Optimized for CPU inference\n",
    "- Various quantization levels (Q4_K_M, Q5_K_S, Q8_0, etc.)\n",
    "- Use with `llama-cpp-python` or similar libraries\n",
    "\n",
    "**Example Hub Search**:\n",
    "- GPTQ models: Search for \"gptq\" in model names\n",
    "- AWQ models: Search for \"awq\" in model names\n",
    "\n",
    "Refer to model cards on HuggingFace Hub for supported quantization variants and detailed loading instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a5c07",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**When to Use Each Quantization Method:**\n",
    "\n",
    "1. **Full Precision (BF16/FP16)**: \n",
    "   - When you have sufficient GPU memory\n",
    "   - Maximum accuracy is critical\n",
    "   - Baseline for comparison\n",
    "\n",
    "2. **8-bit (INT8)**:\n",
    "   - Good balance between memory and quality\n",
    "   - Minimal accuracy degradation\n",
    "   - ~50% memory savings\n",
    "\n",
    "3. **4-bit (NF4)**:\n",
    "   - Maximum memory efficiency\n",
    "   - Acceptable quality for most tasks\n",
    "   - ~75% memory savings\n",
    "   - Enables larger models on smaller GPUs\n",
    "\n",
    "4. **GPTQ/AWQ**:\n",
    "   - Pre-quantized models for production deployment\n",
    "   - Fast inference with optimized kernels\n",
    "   - Available on HuggingFace Hub\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
