{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2789b7a",
   "metadata": {},
   "source": [
    "# LLMs in Practice: Inference with Open-Source Models on Hugging Face\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87685bf7",
   "metadata": {},
   "source": [
    "## Part 1: Introduction to HuggingFace Ecosystem\n",
    "\n",
    "**What is HuggingFace?**\n",
    "- The Hub hosts models, datasets, and Spaces with rich model cards and licensing details.\n",
    "- Transformers provides model architectures, tokenizers, and generation utilities.\n",
    "- Key building blocks: tokenizers (text -> ids), models (ids -> logits), and generation (sampling/decoding).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f99c7c0",
   "metadata": {},
   "source": [
    "### 1.2 Environment Setup\n",
    "\n",
    "Recommended packages:\n",
    "\n",
    "```bash\n",
    "%pip install -U transformers accelerate bitsandbytes huggingface_hub\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4616db89",
   "metadata": {},
   "source": [
    "## 1.3 Authenticate with HuggingFace\n",
    "\n",
    "### Why Authenticate?\n",
    "\n",
    "Authentication with HuggingFace is **optional for most models** but recommended for:\n",
    "- **Gated models**: Some models require approval and authentication (e.g., Llama, Gemma)\n",
    "- **Private models**: Access your own private models or those shared with you\n",
    "- **Upload capabilities**: Push models, datasets, or files to your HuggingFace account\n",
    "\n",
    "### How to Get Your Token\n",
    "\n",
    "Follow these steps to create a HuggingFace access token:\n",
    "\n",
    "1. **Visit HuggingFace**: Go to [https://huggingface.co/](https://huggingface.co/)\n",
    "\n",
    "2. **Navigate to Settings**: \n",
    "   - Click on your profile picture (top right)\n",
    "   - Select **Settings** from the dropdown menu\n",
    "   - Click on **Access Tokens** in the left sidebar\n",
    "   \n",
    "   <img src=\"../assets/image.png\" width=\"250\" alt=\"HuggingFace Access Tokens menu\"/>\n",
    "\n",
    "3. **Create New Token**:\n",
    "   - Click the **\"Create new token\"** button\n",
    "   - Give your token a descriptive name (e.g., \"Tutorial Notebook\")\n",
    "   - Choose token type:\n",
    "     - **Read**: For downloading models only (recommended for this tutorial)\n",
    "     - **Write**: For uploading models/datasets (not needed here)\n",
    "   - Click **\"Create token\"**\n",
    "   \n",
    "   <img src=\"../assets/image-1.png\" width=\"500\" alt=\"Create new token dialog\"/>\n",
    "\n",
    "4. **Copy Your Token**: \n",
    "   - Copy the generated token immediately (it won't be shown again)\n",
    "   - Keep it secure - treat it like a password!\n",
    "\n",
    "### Run the Cell Below\n",
    "\n",
    "Run the next cell and paste your token when prompted. Alternatively, you can set the `HF_TOKEN` environment variable before starting Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff7a86f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c057a39752a47c3b0fcfd5bd4b5f55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Authenticate with HuggingFace\n",
    "# This will prompt you to paste your token in a text box\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()\n",
    "\n",
    "# Alternative: Set token via environment variable\n",
    "# import os\n",
    "# os.environ['HF_TOKEN'] = 'your_token_here'\n",
    "# login(token=os.environ['HF_TOKEN'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57d73040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.11\n",
      "Torch: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA H200\n",
      "Capability: (9, 0)\n",
      "BF16 support: True\n"
     ]
    }
   ],
   "source": [
    "# Check environment\n",
    "import platform\n",
    "import torch\n",
    "\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Capability:\", torch.cuda.get_device_capability(0))\n",
    "    print(\"BF16 support:\", torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75a1451",
   "metadata": {},
   "source": [
    "## Part 2: General Inference with LLMs\n",
    "\n",
    "### 2.1 Define Model Constants and Basic Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d418773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up torch for optimal performance\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.fp32_precision = \"tf32\"\n",
    "    torch.backends.cudnn.conv.fp32_precision = \"tf32\"\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Model identifiers\n",
    "MODEL_INSTRUCT = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "MODEL_THINKING = \"Qwen/Qwen3-4B-Thinking-2507\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cb7801",
   "metadata": {},
   "source": [
    "### 2.2 Load Model and Tokenizer\n",
    "\n",
    "Load the instruct model directly for basic inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb996130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272ab80966944daaaab958064e9f53f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import transformers components\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = MODEL_INSTRUCT\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2646c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04033270",
   "metadata": {},
   "source": [
    "### 2.3 Basic Chat Inference\n",
    "\n",
    "Create helper functions for formatting messages and generating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d534da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_messages(user_prompt, system_prompt=\"You are a helpful assistant.\"):\n",
    "    \"\"\"Format messages for chat models.\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "\n",
    "def generate_chat(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Generate a chat response.\"\"\"\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **kwargs,\n",
    "    )\n",
    "    if do_sample:\n",
    "        gen_kwargs.update({\"temperature\": temperature, \"top_p\": top_p})\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(**gen_kwargs)\n",
    "\n",
    "    gen_ids = output_ids[0, input_ids.shape[-1] :]\n",
    "    return tokenizer.decode(gen_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3690db2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The HuggingFace Hub is a platform that hosts a vast collection of pre-trained models, datasets, and related resources for natural language processing and machine learning. It enables researchers and developers to share, discover, and use models easily through an open, community-driven ecosystem.\n"
     ]
    }
   ],
   "source": [
    "# Try a basic chat completion\n",
    "messages = format_messages(\n",
    "    \"Summarize the HuggingFace Hub in 2 sentences.\",\n",
    "    system_prompt=\"You are a concise assistant.\",\n",
    ")\n",
    "\n",
    "print(generate_chat(model, tokenizer, messages, max_new_tokens=120))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c35c0b",
   "metadata": {},
   "source": [
    "### 2.4 Streaming Responses\n",
    "\n",
    "For longer responses, streaming provides better UX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689fcd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import streaming utilities\n",
    "from threading import Thread\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "\n",
    "def stream_chat(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "):\n",
    "    \"\"\"Stream chat responses token by token.\"\"\"\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)\n",
    "    generation_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "    for text in streamer:\n",
    "        print(text, end=\"\", flush=True)\n",
    "    thread.join()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4f4b868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Write a short poem about GPUs and data centers.\n",
      "assistant\n",
      "In silent halls, where wires hum and flow,  \n",
      "Lies the GPU, sharp as a thought's first glow.  \n",
      "It crunches data, vast and deep,  \n",
      "With thousands of cores, a thunderous leap.  \n",
      "\n",
      "In data centers, rows stretch wide and long,  \n",
      "Where silence speaks through circuits, strong.  \n",
      "Each card computes, a thousand tasks,  \n",
      "From images to AI's vast, hidden pass.  \n",
      "\n",
      "No human eye can trace the flight—  \n",
      "Of numbers dancing in the night.  \n",
      "Yet in the glow of servers' light,  \n",
      "A world of knowledge takes flight.\n"
     ]
    }
   ],
   "source": [
    "messages = format_messages(\"Write a short poem about GPUs and data centers.\")\n",
    "stream_chat(model, tokenizer, messages, max_new_tokens=120, temperature=0.8, top_p=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fc066c",
   "metadata": {},
   "source": [
    "### 2.5 Practical Tasks\n",
    "\n",
    "Test the model on various common LLM tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67366250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== translation ===\n",
      "El modelo se escala eficientemente en GPUs modernas.\n",
      "\n",
      "=== summarization ===\n",
      "- Hugging Face offers open-source natural language processing (NLP) libraries, such as Transformers, enabling easy access to state-of-the-art NLP models.  \n",
      "- It hosts a vast model hub with pre-trained models across various tasks, facilitating rapid experimentation and deployment.  \n",
      "- Provides comprehensive tools for model training, fine-tuning, and production deployment, supporting both research and real-world applications.\n",
      "\n",
      "=== general_qa ===\n",
      "The main purpose of the Transformers library is to provide pre-trained models and tools for working with **transformer-based models** in natural language processing (NLP) and other AI tasks. It simplifies the process of using state-of-the-art models like BERT, GPT, T5, and others by offering:\n",
      "\n",
      "- Pre-trained models that can be fine-tuned for specific tasks (e.g., text classification, translation, summarization).\n",
      "- Easy-to-use APIs for encoding text, generating predictions, and training custom models.\n",
      "- Support for multiple architectures and tasks, including sequence-to-sequence, classification, and generation.\n",
      "- Integration with popular deep learning frameworks like PyTorch and TensorFlow.\n",
      "\n",
      "In short, the Transformers library enables developers and researchers to leverage powerful transformer models with minimal code and efficient workflows.\n",
      "\n",
      "=== planning ===\n",
      "Certainly! Here's a clear, actionable 4-step rollout plan for an internal Large Language Model (LLM) pilot within a company:\n",
      "\n",
      "---\n",
      "\n",
      "**4-Step Rollout Plan for an Internal LLM Pilot**\n",
      "\n",
      "**Step 1: Define Objectives & Identify Use Cases (Weeks 1–2)**  \n",
      "*Goal: Clarify what problems the LLM will solve and how it will create value.*  \n",
      "- Conduct stakeholder interviews with key departments (e.g., HR, Sales, Support, Product) to identify pain points.  \n",
      "- Define specific, measurable goals (e.g., reduce email response time by 30%, generate 50% of routine reports automatically).  \n",
      "- Prioritize 2–3 high-impact, low-risk use cases (e.g., drafting meeting summaries, creating onboarding content, answering FAQs).  \n",
      "- Establish KPIs to track success (e.g., time saved, user satisfaction, accuracy rate).  \n",
      "\n",
      "*Deliverable:* A\n",
      "\n",
      "=== code_explanation ===\n",
      "The Python code:\n",
      "\n",
      "```python\n",
      "for i in range(3):\n",
      "    print(i * i)\n",
      "```\n",
      "\n",
      "does the following:\n",
      "\n",
      "### Step-by-step explanation:\n",
      "\n",
      "1. **`range(3)`**  \n",
      "   This creates a sequence of numbers from `0` to `2` (not including `3`).  \n",
      "   So, `range(3)` generates: `0, 1, 2`\n",
      "\n",
      "2. **`for i in range(3):`**  \n",
      "   This loop iterates over each value in that sequence.  \n",
      "   The variable `i` takes on the values `0`, `1`, and `2` in succession.\n",
      "\n",
      "3. **`print(i * i)`**  \n",
      "   For each value of `i`, it computes `i` squared (i.e., `i * i`) and prints the result.\n",
      "\n",
      "---\n",
      "\n",
      "### What it outputs:\n",
      "\n",
      "- When `i = 0`: `0 * 0 = 0` → prints `0`\n",
      "\n",
      "\n",
      "=== code_generation ===\n",
      "Sure! Here's a simple Python function that checks if a string is a palindrome:\n",
      "\n",
      "```python\n",
      "def is_palindrome(s):\n",
      "    \"\"\"\n",
      "    Check if a string is a palindrome (reads the same forwards and backwards).\n",
      "    \n",
      "    Args:\n",
      "        s (str): The string to check.\n",
      "    \n",
      "    Returns:\n",
      "        bool: True if the string is a palindrome, False otherwise.\n",
      "    \"\"\"\n",
      "    # Remove non-alphanumeric characters and convert to lowercase\n",
      "    cleaned = ''.join(char.lower() for char in s if char.isalnum())\n",
      "    \n",
      "    # Check if the cleaned string reads the same forwards and backwards\n",
      "    return cleaned == cleaned[::-1]\n",
      "\n",
      "# Example usage:\n",
      "print(is_palindrome(\"A man, a plan, a canal: Panama\"))  # True\n",
      "print(is_palindrome(\"race a car\"))                     # False\n",
      "print(is_palindrome(\"madam\"))                         # True\n",
      "```\n",
      "\n",
      "### How it works:\n",
      "- The function removes all non-alphanumeric characters (like spaces, commas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run various practical tasks\n",
    "practical_tasks = {\n",
    "    \"translation\": \"Translate to Spanish: The model scales efficiently on modern GPUs.\",\n",
    "    \"summarization\": (\n",
    "        \"Summarize in 3 bullets: HuggingFace provides open-source NLP libraries, a model hub, \"\n",
    "        \"and tools for training and deployment across research and production.\"\n",
    "    ),\n",
    "    \"general_qa\": \"Q: What is the main purpose of the Transformers library?\",\n",
    "    \"planning\": \"Plan a 4-step rollout for an internal LLM pilot at a company.\",\n",
    "    \"code_explanation\": \"Explain what this Python does: for i in range(3): print(i*i)\",\n",
    "    \"code_generation\": \"Write a Python function that checks if a string is a palindrome.\",\n",
    "}\n",
    "\n",
    "for name, prompt in practical_tasks.items():\n",
    "    messages = format_messages(prompt)\n",
    "    output = generate_chat(model, tokenizer, messages, max_new_tokens=200)\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(output)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd57a4db",
   "metadata": {},
   "source": [
    "### 2.6 Experiment: Compare Generation Parameters\n",
    "\n",
    "See how temperature and sampling affect outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5b51364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== deterministic ===\n",
      "Imagine a global network of brilliant minds and cutting-edge AI working hand-in-hand to solve humanity’s most pressing challenges—from climate resilience to equitable access to healthcare. Together, we’re not just building smarter AI—we’re shaping a future where innovation serves everyone.\n",
      "\n",
      "=== balanced ===\n",
      "Imagine a global community of scientists, engineers, and thinkers united by a shared mission to unlock the next frontier of artificial intelligence—where open collaboration fuels breakthroughs no single mind could achieve alone. Together, we’re building not just smarter AI, but a more transparent, ethical, and human-centered future.\n",
      "\n",
      "=== creative ===\n",
      "Imagine a future where groundbreaking AI innovations emerge from the synergy of top minds, cutting-edge computing, and open collaboration. Our AI research lab brings together scientists, engineers, and visionaries to tackle the biggest challenges—building intelligent systems that are not just powerful, but ethical, transparent, and designed for humanity's benefit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare different generation settings\n",
    "prompt = \"Write a two-sentence pitch for a collaborative AI research lab.\"\n",
    "\n",
    "settings = [\n",
    "    {\"name\": \"deterministic\", \"do_sample\": False},\n",
    "    {\"name\": \"balanced\", \"do_sample\": True, \"temperature\": 0.7, \"top_p\": 0.9},\n",
    "    {\"name\": \"creative\", \"do_sample\": True, \"temperature\": 1.1, \"top_p\": 0.95},\n",
    "]\n",
    "\n",
    "for cfg in settings:\n",
    "    messages = format_messages(prompt, system_prompt=\"You are a marketing copywriter.\")\n",
    "    output = generate_chat(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        messages,\n",
    "        max_new_tokens=120,\n",
    "        **{k: v for k, v in cfg.items() if k != \"name\"},\n",
    "    )\n",
    "    print(f\"=== {cfg['name']} ===\")\n",
    "    print(output)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493d703",
   "metadata": {},
   "source": [
    "### 2.7 Baseline Tests for Model Comparison\n",
    "\n",
    "Run these tests to compare with thinking models later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4c00eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline results saved for comparison with thinking model.\n"
     ]
    }
   ],
   "source": [
    "# Run comparison prompts on non-thinking model\n",
    "comparison_prompts = {\n",
    "    \"math\": \"Solve: If a train travels 120 km in 1.5 hours, what is its average speed?\",\n",
    "    \"reasoning\": \"You have 3 boxes: apples, oranges, and mixed. All labels are wrong. \"\n",
    "    \"Pick one fruit to identify all boxes. Explain.\",\n",
    "    \"analysis\": \"Compare pros and cons of deploying an LLM on-prem vs in the cloud.\",\n",
    "}\n",
    "\n",
    "non_thinking_results = {}\n",
    "for name, prompt in comparison_prompts.items():\n",
    "    messages = format_messages(prompt, system_prompt=\"You are a precise assistant.\")\n",
    "    non_thinking_results[name] = generate_chat(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        messages,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "print(\"Baseline results saved for comparison with thinking model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1343b968",
   "metadata": {},
   "source": [
    "## Part 3: Advanced Reasoning with Thinking Models\n",
    "\n",
    "Thinking models expose internal reasoning steps for complex tasks. Let's switch to the thinking version and compare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd646551",
   "metadata": {},
   "source": [
    "### 3.1 Unload Current Model and Load Thinking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0201bb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# Free up memory before loading the thinking model\n",
    "import gc\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Memory cleared.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c855fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11528e99318e4ad687c82c69e5adf6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the thinking model\n",
    "\n",
    "model_id = MODEL_THINKING\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bd55816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83abd644",
   "metadata": {},
   "source": [
    "### 3.2 Run Same Tests with Thinking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9e1b074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking model results collected.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run same prompts with thinking model\n",
    "thinking_results = {}\n",
    "for name, prompt in comparison_prompts.items():\n",
    "    messages = format_messages(prompt, system_prompt=\"You are a reasoning assistant.\")\n",
    "    thinking_results[name] = generate_chat(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        messages,\n",
    "        max_new_tokens=240,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "print(\"Thinking model results collected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1b9762",
   "metadata": {},
   "source": [
    "### 3.3 Compare Results Side-by-Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deaafa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== math ===\n",
      "Non-thinking:\n",
      "To find the **average speed** of the train, use the formula:\n",
      "\n",
      "$$\n",
      "\\text{Average Speed} = \\frac{\\text{Total Distance}}{\\text{Total Time}}\n",
      "$$\n",
      "\n",
      "Given:\n",
      "- Distance = 120 km\n",
      "- Time = 1.5 hours\n",
      "\n",
      "$$\n",
      "\\text{Average Speed} = \\frac{120 \\text{ km}}{1.5 \\text{ hours}} = 80 \\text{ km/h}\n",
      "$$\n",
      "\n",
      "### ✅ Answer: **80 km/h**\n",
      "\n",
      "Thinking:\n",
      "Okay, let's see. The problem is asking for the average speed of a train that travels 120 km in 1.5 hours. Hmm, average speed... I remember that average speed is calculated by dividing the total distance traveled by the total time taken. So the formula should be speed equals distance divided by time. Let me write that down to be sure.\n",
      "\n",
      "The formula for average speed (v) is:\n",
      "\n",
      "v = d / t\n",
      "\n",
      "where d is distance and t is time.\n",
      "\n",
      "In this case, the distance d is 120 km, and the time t is 1.5 hours. So I need to plug those values into the formula.\n",
      "\n",
      "Let me do the division: 120 km divided by 1.5 hours. Let me think, how do I calculate that? Well, 1.5 is the same as 3/2, so dividing by 3/2 is the same as multiplying by 2/3. So 120 times 2/3. Let's see, 120 divided by 3 is 40, and 40 times 2 is 80. So\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== reasoning ===\n",
      "Non-thinking:\n",
      "This is a classic logic puzzle! Let's break it down step by step.\n",
      "\n",
      "### Given:\n",
      "- There are **3 boxes**:  \n",
      "  - Apples  \n",
      "  - Oranges  \n",
      "  - Mixed (apples and oranges)  \n",
      "- **All labels are wrong**.  \n",
      "- Your goal: **Pick just one fruit** (from one box) to correctly identify the contents of **all three boxes**.\n",
      "\n",
      "---\n",
      "\n",
      "### Step-by-step reasoning:\n",
      "\n",
      "Since **all labels are wrong**, we know:\n",
      "- The box labeled \"Apples\" does **not** contain only apples.\n",
      "- The box labeled \"Oranges\" does **not** contain only oranges.\n",
      "- The box labeled \"Mixed\" does **not** contain a mix.\n",
      "\n",
      "So, we can use this contradiction to our advantage.\n",
      "\n",
      "---\n",
      "\n",
      "### Strategy:\n",
      "Pick **one fruit** from the box labeled **\"Mixed\"**.\n",
      "\n",
      "Why this box?\n",
      "\n",
      "Because it's labeled \"Mixed\", but **all labels are wrong**, so it **cannot**\n",
      "\n",
      "Thinking:\n",
      "Okay, so I need to figure out this problem where there are three boxes: apples, oranges, and mixed. But all the labels are wrong. The goal is to pick just one fruit from one box to correctly identify all the boxes. Hmm, let me think.\n",
      "\n",
      "First, let me visualize the setup. Normally, you'd expect each box to have a label that matches its contents. But here, all labels are incorrect. So the \"apples\" box doesn't have apples, the \"oranges\" box doesn't have oranges, and the \"mixed\" box doesn't have a mix. Wait, actually, the problem says \"all labels are wrong,\" so each label is incorrect. So the box labeled \"apples\" has either oranges or mixed, the one labeled \"oranges\" has either apples or mixed, and the one labeled \"mixed\" has either apples or oranges.\n",
      "\n",
      "The classic problem like this usually involves picking from the box labeled mixed because it's the only one that's not a pure fruit, but wait, no. Wait, in the standard problem, you have three boxes: one with all apples, one with all oranges, and one with a mix. All\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== analysis ===\n",
      "Non-thinking:\n",
      "Deploying a Large Language Model (LLM) either **on-premises (on-prem)** or **in the cloud** each comes with distinct advantages and disadvantages. The choice depends on organizational needs such as data sensitivity, compliance, cost, performance, and scalability. Below is a structured comparison of the **pros and cons** of each approach:\n",
      "\n",
      "---\n",
      "\n",
      "### ✅ **On-Premise Deployment (On-Prem)**\n",
      "\n",
      "#### **Pros:**\n",
      "1. **Data Privacy & Security**  \n",
      "   - Sensitive data (e.g., healthcare, financial, legal) remains within your organization’s network.  \n",
      "   - No data exposure to third-party providers.  \n",
      "   - Full control over access, encryption, and audit logs.\n",
      "\n",
      "2. **Compliance & Regulatory Alignment**  \n",
      "   - Easier to meet industry-specific regulations (e.g., HIPAA, GDPR, PCI-DSS, SOX).  \n",
      "   - Avoids compliance risks associated with cloud data residency or cross-border data\n",
      "\n",
      "Thinking:\n",
      "Okay, the user wants me to compare the pros and cons of deploying an LLM on-prem versus in the cloud. Let me start by recalling the key differences between these two approaches. \n",
      "\n",
      "First, I should consider who the user might be. They could be a tech decision-maker, maybe from a company that's evaluating where to host their AI models. They might be concerned about security, cost, compliance, or scalability. Since they're asking about LLMs specifically, they're probably dealing with large language models which have high resource demands.\n",
      "\n",
      "Hmm, the user didn't specify their industry or company size. That's important because a small startup versus a large enterprise would have different needs. For example, a startup might prioritize cost and simplicity, while a financial institution would care more about compliance and data privacy. I should cover both scenarios in the comparison.\n",
      "\n",
      "Let me break down the main factors. Security and compliance are big ones. On-prem gives full control over data, which is crucial for industries like healthcare or finance where regulations (HIPAA, GDPR) are strict. But cloud providers have strong security measures too, so I need to mention that. \n",
      "\n",
      "Cost is another angle. On-pre\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display comparison\n",
    "for name in comparison_prompts:\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(\"Non-thinking:\")\n",
    "    print(non_thinking_results[name])\n",
    "    print(\"\\nThinking:\")\n",
    "    print(thinking_results[name])\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac838bc7",
   "metadata": {},
   "source": [
    "### 3.4 Exercise: Try Your Own Reasoning Tasks (WIP)\n",
    "\n",
    "Experiment with harder reasoning problems:\n",
    "- Multi-step planning challenges\n",
    "- Complex math word problems\n",
    "- Evaluation and critique of solutions\n",
    "- Logic puzzles\n",
    "\n",
    "Create your own prompts below and compare the two models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ea55691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiments here\n",
    "# Example:\n",
    "# my_prompt = \"...\"\n",
    "# messages = format_messages(my_prompt)\n",
    "# result = generate_chat(model, tokenizer, messages, max_new_tokens=300)\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d66bd1a",
   "metadata": {},
   "source": [
    "## Conclusion (WIP)\n",
    "\n",
    "\n",
    "### Next Step\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
