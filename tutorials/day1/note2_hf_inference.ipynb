{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2789b7a",
   "metadata": {},
   "source": [
    "# LLMs in Practice: Inference with Open-Source Models on Hugging Face\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87685bf7",
   "metadata": {},
   "source": [
    "## Part 1: Introduction to HuggingFace Ecosystem\n",
    "\n",
    "**What is HuggingFace?**\n",
    "- The Hub hosts models, datasets, and Spaces with rich model cards and licensing details.\n",
    "- Transformers provides model architectures, tokenizers, and generation utilities.\n",
    "- Key building blocks: tokenizers (text -> ids), models (ids -> logits), and generation (sampling/decoding).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f99c7c0",
   "metadata": {},
   "source": [
    "### 1.2 Environment Setup\n",
    "\n",
    "Recommended packages:\n",
    "\n",
    "```bash\n",
    "%pip install -U transformers accelerate bitsandbytes huggingface_hub\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4616db89",
   "metadata": {},
   "source": [
    "## 1.3 Authenticate with HuggingFace\n",
    "\n",
    "### Why Authenticate?\n",
    "\n",
    "Authentication with HuggingFace is **optional for most models** but recommended for:\n",
    "- **Gated models**: Some models require approval and authentication (e.g., Llama, Gemma)\n",
    "- **Private models**: Access your own private models or those shared with you\n",
    "- **Upload capabilities**: Push models, datasets, or files to your HuggingFace account\n",
    "\n",
    "### How to Get Your Token\n",
    "\n",
    "Follow these steps to create a HuggingFace access token:\n",
    "\n",
    "1. **Visit HuggingFace**: Go to [https://huggingface.co/](https://huggingface.co/)\n",
    "\n",
    "2. **Navigate to Settings**: \n",
    "   - Click on your profile picture (top right)\n",
    "   - Select **Settings** from the dropdown menu\n",
    "   - Click on **Access Tokens** in the left sidebar\n",
    "   \n",
    "   <img src=\"../assets/image.png\" width=\"250\" alt=\"HuggingFace Access Tokens menu\"/>\n",
    "\n",
    "3. **Create New Token**:\n",
    "   - Click the **\"Create new token\"** button\n",
    "   - Give your token a descriptive name (e.g., \"Tutorial Notebook\")\n",
    "   - Choose token type:\n",
    "     - **Read**: For downloading models only (recommended for this tutorial)\n",
    "     - **Write**: For uploading models/datasets (not needed here)\n",
    "   - Click **\"Create token\"**\n",
    "   \n",
    "   <img src=\"../assets/image-1.png\" width=\"500\" alt=\"Create new token dialog\"/>\n",
    "\n",
    "4. **Copy Your Token**: \n",
    "   - Copy the generated token immediately (it won't be shown again)\n",
    "   - Keep it secure - treat it like a password!\n",
    "\n",
    "### Run the Cell Below\n",
    "\n",
    "Run the next cell and paste your token when prompted. Alternatively, you can set the `HF_TOKEN` environment variable before starting Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff7a86f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545a02f508804c03b5324d4cc7880190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Authenticate with HuggingFace\n",
    "# This will prompt you to paste your token in a text box\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()\n",
    "\n",
    "# Alternative: Set token via environment variable\n",
    "# import os\n",
    "# os.environ['HF_TOKEN'] = 'your_token_here'\n",
    "# login(token=os.environ['HF_TOKEN'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57d73040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.12\n",
      "Torch: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "Capability: (7, 5)\n",
      "BF16 support: True\n"
     ]
    }
   ],
   "source": [
    "# Check environment\n",
    "import platform\n",
    "import torch\n",
    "\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Capability:\", torch.cuda.get_device_capability(0))\n",
    "    print(\"BF16 support:\", torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75a1451",
   "metadata": {},
   "source": [
    "## Part 2: General Inference with LLMs\n",
    "\n",
    "### 2.1 Define Model Constants and Basic Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d418773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up torch for optimal performance\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.fp32_precision = \"tf32\"\n",
    "    torch.backends.cudnn.conv.fp32_precision = \"tf32\"\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Model identifiers\n",
    "MODEL_INSTRUCT = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "MODEL_THINKING = \"Qwen/Qwen3-4B-Thinking-2507\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cb7801",
   "metadata": {},
   "source": [
    "### 2.2 Load Model and Tokenizer\n",
    "\n",
    "Load the instruct model directly for basic inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb996130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f89af106544c2295f7655e227ca0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f08e89d14642aebea149ce65c73688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6899c3a3a57446f3a08d00b84b0243fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f58f9f639d34a9ca1ecf2e8ac8f63f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7185eaf87c1c45dbb82800836d730535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d969cd2ca64a48bea1ee6bf310f881a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffbb42b6a2d40c490f879792f7d6035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81686d5d6974db7b4ac205ba4a2719e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197a9808bb714e9d9fd51a7507340fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e751bd54435a4a93bd94a00a816218f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498fdb0bce8744f8a65f8c0343fb7dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ba7dc6ab0547be8b83a2989cae7b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import transformers components\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = MODEL_INSTRUCT\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2646c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04033270",
   "metadata": {},
   "source": [
    "### 2.3 Basic Chat Inference\n",
    "\n",
    "Create helper functions for formatting messages and generating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70d534da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_messages(user_prompt, system_prompt=\"You are a helpful assistant.\"):\n",
    "    \"\"\"Format messages for chat models.\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "\n",
    "def generate_chat(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Generate a chat response.\"\"\"\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **kwargs,\n",
    "    )\n",
    "    if do_sample:\n",
    "        gen_kwargs.update({\"temperature\": temperature, \"top_p\": top_p})\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(**gen_kwargs)\n",
    "\n",
    "    gen_ids = output_ids[0, input_ids.shape[-1] :]\n",
    "    return tokenizer.decode(gen_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3690db2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hugging Face Hub is a platform that hosts a vast collection of pre-trained models, datasets, and related resources for natural language processing and machine learning. It enables researchers and developers to share, discover, and use models easily through an open, community-driven ecosystem.\n"
     ]
    }
   ],
   "source": [
    "# Try a basic chat completion\n",
    "messages = format_messages(\n",
    "    \"Summarize the HuggingFace Hub in 2 sentences.\",\n",
    "    system_prompt=\"You are a concise assistant.\",\n",
    ")\n",
    "\n",
    "print(generate_chat(model, tokenizer, messages, max_new_tokens=120))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c35c0b",
   "metadata": {},
   "source": [
    "### 2.4 Streaming Responses\n",
    "\n",
    "For longer responses, streaming provides better UX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "689fcd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import streaming utilities\n",
    "from threading import Thread\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "\n",
    "def stream_chat(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "):\n",
    "    \"\"\"Stream chat responses token by token.\"\"\"\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)\n",
    "    generation_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "    for text in streamer:\n",
    "        print(text, end=\"\", flush=True)\n",
    "    thread.join()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4f4b868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Write a short poem about GPUs and data centers.\n",
      "assistant\n",
      "In silent halls, where wires hum and flow,  \n",
      "Lies the GPU, sharp as a thought's first glow.  \n",
      "A thousand cores, in parallel dance,  \n",
      "Crunching data like a digital trance.  \n",
      "\n",
      "Data centers rise, a city built of light,  \n",
      "Where servers stand, their circuits bright.  \n",
      "Through thundering streams of binary streams,  \n",
      "They feed the world with thought and dreams.  \n",
      "\n",
      "From AI that learns to art that's born,  \n",
      "To weather forecasts, vast and warm—  \n",
      "Each pixel, line, each model trained,  \n",
      "Is powered by the GPU’s silent\n"
     ]
    }
   ],
   "source": [
    "messages = format_messages(\"Write a short poem about GPUs and data centers.\")\n",
    "stream_chat(model, tokenizer, messages, max_new_tokens=120, temperature=0.8, top_p=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fc066c",
   "metadata": {},
   "source": [
    "### 2.5 Practical Tasks\n",
    "\n",
    "Test the model on various common LLM tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67366250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== translation ===\n",
      "El modelo se escala eficientemente en GPUs modernas.\n",
      "\n",
      "=== summarization ===\n",
      "- Hugging Face offers open-source NLP libraries, such as Transformers, enabling easy access to state-of-the-art natural language processing models.  \n",
      "- It hosts a vast model hub with pre-trained models across various tasks, facilitating rapid development and experimentation.  \n",
      "- Provides comprehensive tools for training, fine-tuning, and deploying NLP models, supporting both research and production environments.\n",
      "\n",
      "=== general_qa ===\n",
      "The main purpose of the Transformers library is to provide pre-trained models and tools for working with **transformer-based models** in natural language processing (NLP) and other AI applications. Specifically, it enables developers and researchers to:\n",
      "\n",
      "- Easily use and fine-tune powerful pre-trained models (like BERT, GPT, T5, RoBERTa, etc.) for tasks such as text classification, named entity recognition, machine translation, question answering, and more.\n",
      "- Simplify the implementation of transformer architectures through high-level APIs and components (e.g., tokenizers, models, trainers).\n",
      "- Support both **text-to-text** and **text-to-speech** tasks with consistent interfaces.\n",
      "- Accelerate development by offering ready-to-use models, training pipelines, and evaluation tools.\n",
      "\n",
      "Developed by **Hugging Face**, the Transformers library is widely used in both academic and industrial settings due to its ease of use, extensive model support, and strong community ecosystem.\n",
      "\n",
      "=== planning ===\n",
      "Certainly! Here's a clear, actionable **4-step rollout plan** for an internal Large Language Model (LLM) pilot at a company, designed to ensure success, minimize risk, and gather valuable feedback:\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 1: Define Objectives & Scope (Weeks 1–2)**\n",
      "**Goal:** Establish clear purpose, success metrics, and boundaries for the pilot.\n",
      "\n",
      "- **Identify use cases:** Choose 2–3 high-impact, realistic internal applications (e.g., drafting emails, summarizing meeting notes, generating code snippets, answering HR FAQs).\n",
      "- **Set measurable KPIs:** Define success criteria (e.g., \"Reduce meeting prep time by 30%,\" \"Improve response time to employee queries by 50%\").\n",
      "- **Determine pilot boundaries:** Limit scope to specific departments (e.g., sales, support, product) or functions to avoid overreach.\n",
      "- **Form a pilot steering committee:** Include representatives from IT,\n",
      "\n",
      "=== code_explanation ===\n",
      "The Python code:\n",
      "\n",
      "```python\n",
      "for i in range(3):\n",
      "    print(i * i)\n",
      "```\n",
      "\n",
      "does the following:\n",
      "\n",
      "### Step-by-step explanation:\n",
      "\n",
      "1. **`range(3)`**  \n",
      "   This creates a sequence of numbers from `0` to `2` (not including `3`).  \n",
      "   So, `range(3)` generates: `0, 1, 2`\n",
      "\n",
      "2. **`for i in range(3):`**  \n",
      "   This loop iterates over each value in that sequence.  \n",
      "   The variable `i` takes on the values `0`, `1`, and `2` one at a time.\n",
      "\n",
      "3. **`print(i * i)`**  \n",
      "   For each value of `i`, it computes `i * i` (the square of `i`) and prints it.\n",
      "\n",
      "---\n",
      "\n",
      "### What it outputs:\n",
      "\n",
      "- When `i = 0`: `0 * 0 = 0` → prints `0`\n",
      "\n",
      "\n",
      "=== code_generation ===\n",
      "Sure! Here's a Python function that checks if a string is a palindrome:\n",
      "\n",
      "```python\n",
      "def is_palindrome(s):\n",
      "    \"\"\"\n",
      "    Check if a string is a palindrome (reads the same forwards and backwards).\n",
      "    \n",
      "    Args:\n",
      "        s (str): The string to check.\n",
      "    \n",
      "    Returns:\n",
      "        bool: True if the string is a palindrome, False otherwise.\n",
      "    \"\"\"\n",
      "    # Remove non-alphanumeric characters and convert to lowercase\n",
      "    cleaned = ''.join(char.lower() for char in s if char.isalnum())\n",
      "    \n",
      "    # Check if the cleaned string reads the same forwards and backwards\n",
      "    return cleaned == cleaned[::-1]\n",
      "\n",
      "# Example usage:\n",
      "print(is_palindrome(\"A man, a plan, a canal: Panama\"))  # True\n",
      "print(is_palindrome(\"race a car\"))                     # False\n",
      "print(is_palindrome(\"madam\"))                         # True\n",
      "```\n",
      "\n",
      "### How it works:\n",
      "- It strips out non-alphanumeric characters (like spaces, punctuation) and\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run various practical tasks\n",
    "practical_tasks = {\n",
    "    \"translation\": \"Translate to Spanish: The model scales efficiently on modern GPUs.\",\n",
    "    \"summarization\": (\n",
    "        \"Summarize in 3 bullets: HuggingFace provides open-source NLP libraries, a model hub, \"\n",
    "        \"and tools for training and deployment across research and production.\"\n",
    "    ),\n",
    "    \"general_qa\": \"Q: What is the main purpose of the Transformers library?\",\n",
    "    \"planning\": \"Plan a 4-step rollout for an internal LLM pilot at a company.\",\n",
    "    \"code_explanation\": \"Explain what this Python does: for i in range(3): print(i*i)\",\n",
    "    \"code_generation\": \"Write a Python function that checks if a string is a palindrome.\",\n",
    "}\n",
    "\n",
    "for name, prompt in practical_tasks.items():\n",
    "    messages = format_messages(prompt)\n",
    "    output = generate_chat(model, tokenizer, messages, max_new_tokens=200)\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(output)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd57a4db",
   "metadata": {},
   "source": [
    "### 2.6 Experiment: Compare Generation Parameters\n",
    "\n",
    "See how temperature and sampling affect outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5b51364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== deterministic ===\n",
      "Imagine a global network of brilliant minds and cutting-edge AI working hand-in-hand to solve humanity’s most pressing challenges—from climate resilience to equitable access to healthcare. Together, we’re not just building smarter AI—we’re shaping a future where innovation serves everyone.\n",
      "\n",
      "=== balanced ===\n",
      "Imagine a global network of scientists, engineers, and thinkers united by a shared vision: to build AI that doesn’t just solve problems—but understands them. At our collaborative AI research lab, breakthroughs happen not in isolation, but through open dialogue, diverse perspectives, and relentless curiosity.\n",
      "\n",
      "=== creative ===\n",
      "Imagine a global community of scientists, engineers, and thinkers uniting in a cutting-edge AI research lab where curiosity drives innovation and diversity fuels breakthroughs. Together, we're not just building smarter algorithms—we're shaping a future where AI serves humanity with integrity, inclusivity, and purpose.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare different generation settings\n",
    "prompt = \"Write a two-sentence pitch for a collaborative AI research lab.\"\n",
    "\n",
    "settings = [\n",
    "    {\"name\": \"deterministic\", \"do_sample\": False},\n",
    "    {\"name\": \"balanced\", \"do_sample\": True, \"temperature\": 0.7, \"top_p\": 0.9},\n",
    "    {\"name\": \"creative\", \"do_sample\": True, \"temperature\": 1.1, \"top_p\": 0.95},\n",
    "]\n",
    "\n",
    "for cfg in settings:\n",
    "    messages = format_messages(prompt, system_prompt=\"You are a marketing copywriter.\")\n",
    "    output = generate_chat(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        messages,\n",
    "        max_new_tokens=120,\n",
    "        **{k: v for k, v in cfg.items() if k != \"name\"},\n",
    "    )\n",
    "    print(f\"=== {cfg['name']} ===\")\n",
    "    print(output)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493d703",
   "metadata": {},
   "source": [
    "### 2.7 Baseline Tests for Model Comparison\n",
    "\n",
    "Run these tests to compare with thinking models later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4c00eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline results saved for comparison with thinking model.\n"
     ]
    }
   ],
   "source": [
    "# Run comparison prompts on non-thinking model\n",
    "comparison_prompts = {\n",
    "    \"math\": \"Solve: If a train travels 120 km in 1.5 hours, what is its average speed?\",\n",
    "    \"reasoning\": \"You have 3 boxes: apples, oranges, and mixed. All labels are wrong. \"\n",
    "    \"Pick one fruit to identify all boxes. Explain.\",\n",
    "    \"analysis\": \"Compare pros and cons of deploying an LLM on-prem vs in the cloud.\",\n",
    "}\n",
    "\n",
    "non_thinking_results = {}\n",
    "for name, prompt in comparison_prompts.items():\n",
    "    messages = format_messages(prompt, system_prompt=\"You are a precise assistant.\")\n",
    "    non_thinking_results[name] = generate_chat(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        messages,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "print(\"Baseline results saved for comparison with thinking model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1343b968",
   "metadata": {},
   "source": [
    "## Part 3: Advanced Reasoning with Thinking Models\n",
    "\n",
    "Thinking models expose internal reasoning steps for complex tasks. Let's switch to the thinking version and compare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd646551",
   "metadata": {},
   "source": [
    "### 3.1 Unload Current Model and Load Thinking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0201bb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# Free up memory before loading the thinking model\n",
    "import gc\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Memory cleared.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c855fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa6f8175421449faa66d51f0abc754a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe40d62d8ad434490281965a9c6758b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9559018cc9524a6dadf898c2b5e471be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864399b54ff54063b42296887e3ebc63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b38b5fd691b4cc484cd1b40a7441d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e72e0e5a7de43629a90dc7f119da651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba2c2015ca141088c01d065ae2fae31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6048b369399c45ce92559b9f94bbcd37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190baa9ec8204f42b2975e9d324a7a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161e399001a048adb148bbebaaba9a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e413077b1654455a700f47effdf4c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62a46d3ca0e44a58fcef679e7390e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# Load the thinking model\n",
    "\n",
    "model_id = MODEL_THINKING\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bd55816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83abd644",
   "metadata": {},
   "source": [
    "### 3.2 Run Same Tests with Thinking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9e1b074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking model results collected.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run same prompts with thinking model\n",
    "thinking_results = {}\n",
    "for name, prompt in comparison_prompts.items():\n",
    "    messages = format_messages(prompt, system_prompt=\"You are a reasoning assistant.\")\n",
    "    thinking_results[name] = generate_chat(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        messages,\n",
    "        max_new_tokens=240,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "print(\"Thinking model results collected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1b9762",
   "metadata": {},
   "source": [
    "### 3.3 Compare Results Side-by-Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deaafa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== math ===\n",
      "Non-thinking:\n",
      "To find the **average speed** of the train, use the formula:\n",
      "\n",
      "$$\n",
      "\\text{Average Speed} = \\frac{\\text{Total Distance}}{\\text{Total Time}}\n",
      "$$\n",
      "\n",
      "Given:\n",
      "- Distance = 120 km\n",
      "- Time = 1.5 hours\n",
      "\n",
      "$$\n",
      "\\text{Average Speed} = \\frac{120 \\text{ km}}{1.5 \\text{ hours}} = 80 \\text{ km/h}\n",
      "$$\n",
      "\n",
      "### ✅ Answer: **80 km/h**\n",
      "\n",
      "Thinking:\n",
      "Okay, let's see. The problem is asking for the average speed of a train that travels 120 km in 1.5 hours. Hmm, average speed... I remember that average speed is calculated by dividing the total distance traveled by the total time taken. So the formula should be speed equals distance divided by time. Let me write that down to be sure.\n",
      "\n",
      "The formula for average speed (v) is:\n",
      "\n",
      "v = d / t\n",
      "\n",
      "where d is distance and t is time.\n",
      "\n",
      "In this case, the distance d is 120 km, and the time t is 1.5 hours. So I need to plug those values into the formula.\n",
      "\n",
      "Let me do the division: 120 km divided by 1.5 hours. Let me think, how do I calculate that? Well, 1.5 hours is the same as 3/2 hours, so dividing by 3/2 is the same as multiplying by 2/3. So 120 times 2/3. Let's compute that.\n",
      "\n",
      "120 divided by 1.5. Let's see, 1.5 times 80 is\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== reasoning ===\n",
      "Non-thinking:\n",
      "This is a classic logic puzzle! Let's break it down step by step.\n",
      "\n",
      "### Given:\n",
      "- There are **3 boxes**:  \n",
      "  - Apples  \n",
      "  - Oranges  \n",
      "  - Mixed (apples and oranges)  \n",
      "- **All labels are wrong**.  \n",
      "- Your goal: **Pick just one fruit** (from one box) to correctly identify the contents of **all three boxes**.\n",
      "\n",
      "---\n",
      "\n",
      "### Step-by-step reasoning:\n",
      "\n",
      "Since **all labels are wrong**, we know:\n",
      "- The box labeled \"Apples\" does **not** contain only apples.\n",
      "- The box labeled \"Oranges\" does **not** contain only oranges.\n",
      "- The box labeled \"Mixed\" does **not** contain a mix.\n",
      "\n",
      "So, we can use this contradiction to our advantage.\n",
      "\n",
      "---\n",
      "\n",
      "### Strategy:\n",
      "Pick **one fruit** from the box labeled **\"Mixed\"**.\n",
      "\n",
      "Why this box?\n",
      "\n",
      "Because it's labeled \"Mixed\", but **all labels are wrong**, so it **cannot**\n",
      "\n",
      "Thinking:\n",
      "Okay, so I need to figure out this problem where there are three boxes: apples, oranges, and mixed. But all the labels are wrong. The goal is to pick just one fruit from one box to correctly identify all the boxes. Hmm, let me think.\n",
      "\n",
      "First, let me visualize the setup. Normally, you'd expect each box to have a label that matches its contents. But here, all labels are incorrect. So the \"apples\" box doesn't have apples, the \"oranges\" box doesn't have oranges, and the \"mixed\" box doesn't have a mix. Wait, actually, the problem says \"all labels are wrong,\" so each label is incorrect. So the box labeled \"apples\" has either oranges or mixed, the one labeled \"oranges\" has either apples or mixed, and the one labeled \"mixed\" has either apples or oranges.\n",
      "\n",
      "The classic problem like this usually involves picking from the box labeled mixed because it's the only one that's not a pure fruit. Wait, but in this case, the mixed box is labeled wrong, so it must be either all apples or all oranges. Wait, no, hold on. Wait,\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== analysis ===\n",
      "Non-thinking:\n",
      "Deploying a Large Language Model (LLM) either **on-premises (on-prem)** or **in the cloud** involves trade-offs across performance, cost, security, scalability, and operational complexity. Below is a structured comparison of the **pros and cons** of each approach:\n",
      "\n",
      "---\n",
      "\n",
      "### ✅ **On-Premise Deployment (Local/On-Prem)**\n",
      "\n",
      "#### **Pros:**\n",
      "1. **Data Privacy & Security:**\n",
      "   - Sensitive data never leaves the organization’s network.\n",
      "   - Full control over data access, encryption, and compliance (e.g., HIPAA, GDPR).\n",
      "   - Ideal for regulated industries (finance, healthcare, government).\n",
      "\n",
      "2. **Compliance & Auditability:**\n",
      "   - Easier to meet regulatory requirements with full visibility and audit logs.\n",
      "   - No third-party data handling or exposure.\n",
      "\n",
      "3. **Customization & Control:**\n",
      "   - Full control over model tuning, fine-tuning, and integration with internal systems.\n",
      "\n",
      "\n",
      "Thinking:\n",
      "Okay, the user wants me to compare the pros and cons of deploying an LLM on-prem versus in the cloud. Let me start by recalling the key differences between these two approaches. \n",
      "\n",
      "First, I should consider who the user might be. They could be a tech decision-maker, maybe from a company that's evaluating where to host their AI models. They might be concerned about security, cost, compliance, or scalability. Since they're asking about LLMs specifically, they're probably dealing with large language models which have high resource demands.\n",
      "\n",
      "Hmm, the user didn't specify their industry or company size. That's important because a small startup versus a large enterprise would have different needs. For example, a startup might prioritize cost and simplicity, while a financial institution would care more about compliance. I should cover both scenarios in the comparison.\n",
      "\n",
      "Let me break down the main factors. Security and compliance are big ones. On-prem gives full control over data, which is crucial for industries like healthcare or finance where data laws (like HIPAA or GDPR) are strict. But cloud providers have strong security measures too, so I need to mention that. \n",
      "\n",
      "Cost is another angle. On-prem\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display comparison\n",
    "for name in comparison_prompts:\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(\"Non-thinking:\")\n",
    "    print(non_thinking_results[name])\n",
    "    print(\"\\nThinking:\")\n",
    "    print(thinking_results[name])\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
